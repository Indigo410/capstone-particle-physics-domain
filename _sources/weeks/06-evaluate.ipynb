{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Week 6 Notebook: Evalulating Model Performance and Robustness\n",
    "===============================================================\n",
    "\n",
    "Let's take a look at the model performance and dependence on other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import uproot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 48 track-level features\n",
    "features = ['track_pt',\n",
    "            'track_ptrel',\n",
    "            'trackBTag_Eta',\n",
    "            'trackBTag_DeltaR',\n",
    "            'trackBTag_EtaRel',\n",
    "            'trackBTag_JetDistVal',\n",
    "            'trackBTag_Momentum',\n",
    "            'trackBTag_PPar',\n",
    "            'trackBTag_PParRatio',\n",
    "            'trackBTag_PtRatio',\n",
    "            'trackBTag_PtRel',\n",
    "            'trackBTag_Sip2dSig',\n",
    "            'trackBTag_Sip2dVal',\n",
    "            'trackBTag_Sip3dSig',\n",
    "            'trackBTag_Sip3dVal',\n",
    "            'track_VTX_ass',\n",
    "            'track_charge',\n",
    "            'track_deltaR',\n",
    "            'track_detadeta',\n",
    "            'track_dlambdadz',\n",
    "            'track_dlambdadz',\n",
    "            'track_dphidphi',\n",
    "            'track_dphidxy',\n",
    "            'track_dptdpt',\n",
    "            'track_drminsv',\n",
    "            'track_drsubjet1',\n",
    "            'track_drsubjet2',\n",
    "            'track_dxy',\n",
    "            'track_dxydxy',\n",
    "            'track_dxydz',\n",
    "            'track_dxysig',\n",
    "            'track_dz',\n",
    "            'track_dzdz',        \n",
    "            'track_dzsig',\n",
    "            'track_erel',\n",
    "            'track_etarel',\n",
    "            'track_fromPV',\n",
    "            'track_isChargedHad',\n",
    "            'track_isEl',\n",
    "            'track_isMu',\n",
    "            'track_lostInnerHits',\n",
    "            'track_mass',\n",
    "            'track_normchi2',            \n",
    "            'track_phirel',\n",
    "            'track_pt',\n",
    "            'track_ptrel',\n",
    "            'track_puppiw',\n",
    "            'track_quality']\n",
    "\n",
    "# spectators to define mass/pT window\n",
    "spectators = ['fj_sdmass',\n",
    "              'fj_pt']\n",
    "\n",
    "# 2 labels: QCD or Hbb (we'll reduce the following labels)\n",
    "labels =  ['label_QCD_b',\n",
    "           'label_QCD_bb',\n",
    "           'label_QCD_c', \n",
    "           'label_QCD_cc', \n",
    "           'label_QCD_others',\n",
    "           'sample_isQCD',\n",
    "           'label_H_bb']\n",
    "\n",
    "nfeatures = len(features)\n",
    "nspectators = len(spectators)\n",
    "nlabels = 2\n",
    "\n",
    "# we're going to zero-pad up to 60 tracks\n",
    "ntracks = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_files, features, labels, spectators, batch_size=1024, n_dim=60, \n",
    "                 remove_mass_pt_window=False, remove_unlabeled=True, return_spectators=False,\n",
    "                 max_entry = 20000):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_files = list_files\n",
    "        self.features = features\n",
    "        self.spectators = spectators\n",
    "        self.return_spectators = return_spectators\n",
    "        self.n_dim = n_dim\n",
    "        self.n_channels = len(features)\n",
    "        self.remove_mass_pt_window = remove_mass_pt_window\n",
    "        self.remove_unlabeled = remove_unlabeled\n",
    "        self.global_IDs = []\n",
    "        self.local_IDs = []\n",
    "        self.file_mapping = []\n",
    "        self.max_entry = max_entry\n",
    "        self.open_files = [None]*len(self.list_files)\n",
    "        running_total = 0\n",
    "        for i, file_name in enumerate(self.list_files):\n",
    "            root_file = uproot.open(file_name)\n",
    "            self.open_files.append(root_file)\n",
    "            tree = root_file['deepntuplizer/tree']\n",
    "            tree_length = min(len(tree),max_entry)\n",
    "            self.global_IDs.append(np.arange(running_total,running_total+tree_length))\n",
    "            self.local_IDs.append(np.arange(tree_length))\n",
    "            self.file_mapping.append(np.repeat(i,tree_length))\n",
    "            running_total += tree_length\n",
    "            root_file.close()\n",
    "        self.global_IDs = np.concatenate(self.global_IDs)\n",
    "        self.local_IDs = np.concatenate(self.local_IDs)\n",
    "        self.file_mapping = np.concatenate(self.file_mapping)\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.global_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        files = self.file_mapping[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        unique_files = np.unique(files)\n",
    "        starts = np.array([min(indexes[files==i]) for i in unique_files])\n",
    "        stops = np.array([max(indexes[files==i]) for i in unique_files])\n",
    "\n",
    "        # Check if files needed open (if not open them)\n",
    "        # Also if file is not needed, close it\n",
    "        for ifile, file_name in enumerate(self.list_files):\n",
    "            if ifile in unique_files:\n",
    "                if self.open_files[ifile] is None: \n",
    "                    self.open_files[ifile] = uproot.open(file_name)\n",
    "            else:\n",
    "                if self.open_files[ifile] is not None: \n",
    "                    self.open_files[ifile].close()\n",
    "                    self.open_files[ifile] = None\n",
    "            \n",
    "        # Generate data\n",
    "        return self.__data_generation(unique_files, starts, stops)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = self.local_IDs\n",
    "\n",
    "    def __data_generation(self, unique_files, starts, stops):\n",
    "        'Generates data containing batch_size samples' \n",
    "        # X : (n_samples, n_dim, n_channels)\n",
    "        # y : (n_samples, 2)\n",
    "        Xs = []\n",
    "        ys = []\n",
    "        zs = []\n",
    "        \n",
    "        # Generate data\n",
    "        for ifile, start, stop in zip(unique_files, starts, stops):\n",
    "            if self.return_spectators:\n",
    "                X, y, z = self.__get_features_labels(ifile, start, stop)\n",
    "                zs.append(z)\n",
    "            else:\n",
    "                X, y = self.__get_features_labels(ifile, start, stop)\n",
    "            Xs.append(X)\n",
    "            ys.append(y)\n",
    "            \n",
    "        # Stack data if going over multiple files\n",
    "        if len(unique_files)>1:\n",
    "            X = np.concatenate(Xs,axis=0)\n",
    "            y = np.concatenate(ys,axis=0)\n",
    "            if self.return_spectators:\n",
    "                z = np.concatenate(zs,axis=0)\n",
    "            \n",
    "        if self.return_spectators:\n",
    "            return X, y, z\n",
    "        \n",
    "        return X, y\n",
    "                         \n",
    "    def __get_features_labels(self, ifile, entrystart, entrystop):\n",
    "        'Loads data from one file'\n",
    "        \n",
    "        # Double check that file is open\n",
    "        if self.open_files[ifile] is None:\n",
    "            root_file = uproot.open(self.list_file[ifile])\n",
    "        else:\n",
    "            root_file = self.open_files[ifile]\n",
    "            \n",
    "        tree = root_file['deepntuplizer/tree']\n",
    "        \n",
    "        feature_array = tree.arrays(branches=self.features, \n",
    "                                    entrystart=entrystart,\n",
    "                                    entrystop=entrystop,\n",
    "                                    namedecode='utf-8')\n",
    "\n",
    "        label_array_all = tree.arrays(branches=self.labels, \n",
    "                                      entrystart=entrystart,\n",
    "                                      entrystop=entrystop,\n",
    "                                      namedecode='utf-8')\n",
    "\n",
    "        X = np.stack([feature_array[feat].pad(self.n_dim, clip=True).fillna(0).regular() for feat in features],axis=2)\n",
    "        n_samples = X.shape[0]\n",
    "    \n",
    "        y = np.zeros((n_samples,2))\n",
    "        y[:,0] = label_array_all['sample_isQCD'] * (label_array_all['label_QCD_b'] + \\\n",
    "                                                    label_array_all['label_QCD_bb'] + \\\n",
    "                                                    label_array_all['label_QCD_c'] + \\\n",
    "                                                    label_array_all['label_QCD_cc'] + \\\n",
    "                                                    label_array_all['label_QCD_others'])\n",
    "        y[:,1] = label_array_all['label_H_bb']\n",
    "\n",
    "        \n",
    "        if self.remove_mass_pt_window:\n",
    "            # remove data outside of mass/pT range\n",
    "            spec_array = tree.arrays(branches=self.spectators, \n",
    "                                     entrystart=entrystart,\n",
    "                                     entrystop=entrystop,\n",
    "                                     namedecode='utf-8')\n",
    "            \n",
    "            z = np.stack([spec_array[spec] for spec in self.spectators],axis=1)\n",
    "            X = X[(z[:,0] > 40) & (z[:,0] < 200) & (z[:,1] > 300) & (z[:,1] < 2000)]\n",
    "            y = y[(z[:,0] > 40) & (z[:,0] < 200) & (z[:,1] > 300) & (z[:,1] < 2000)]\n",
    "            z = z[(z[:,0] > 40) & (z[:,0] < 200) & (z[:,1] > 300) & (z[:,1] < 2000)]\n",
    "                        \n",
    "        if self.remove_unlabeled:\n",
    "            # remove unlabeled data\n",
    "            X = X[np.sum(y,axis=1)==1]\n",
    "            if self.return_spectators:\n",
    "                z = z[np.sum(y,axis=1)==1]\n",
    "            y = y[np.sum(y,axis=1)==1]\n",
    "            \n",
    "        if self.return_spectators:\n",
    "            return X, y, z\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Previous Model\n",
    "Here, we will load the last model trained in Notebook 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callbacks\n",
    "from keras.models import load_model\n",
    "\n",
    "keras_model_conv1d = load_model('keras_model_conv1d_best.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Data Generator for Testing\n",
    "\n",
    "We will configure the data generator to load testing data, including \"spectator variables\" that we were not used in the training, but may be correlated with the output of the algorithm. Specifically, the jet mass and pT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load testing file\n",
    "test_files = ['root://eospublic.cern.ch//eos/opendata/cms/datascience/HiggsToBBNtupleProducerTool/HiggsToBBNTuple_HiggsToBB_QCD_RunII_13TeV_MC/test/ntuple_merged_0.root'\n",
    "             ]\n",
    "test_generator = DataGenerator(test_files, features, labels, spectators, batch_size=8192, n_dim=ntracks, \n",
    "                               remove_mass_pt_window=True, \n",
    "                               remove_unlabeled=True,\n",
    "                               return_spectators=True,\n",
    "                               max_entry = 200000) # basically, no maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/24 [00:10<03:57, 10.34s/it]"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "# run model inference on test data set\n",
    "predict_array_cnn = []\n",
    "label_array_test = []\n",
    "spec_array_test = []\n",
    "\n",
    "for t in tqdm.tqdm(test_generator,total=len(test_generator)):\n",
    "    label_array_test.append(t[1])\n",
    "    spec_array_test.append(t[2])\n",
    "    predict_array_cnn.append(keras_model_conv1d.predict(t[0]))\n",
    "    \n",
    "predict_array_cnn = np.concatenate(predict_array_cnn,axis=0)\n",
    "label_array_test = np.concatenate(label_array_test,axis=0)\n",
    "spec_array_test = np.concatenate(spec_array_test,axis=0)\n",
    "\n",
    "# create ROC curves\n",
    "fpr_cnn, tpr_cnn, threshold_cnn = roc_curve(label_array_test[:,1], predict_array_cnn[:,1])\n",
    "    \n",
    "# plot ROC curves\n",
    "plt.figure()\n",
    "plt.plot(tpr_cnn, fpr_cnn, lw=2.5, label=\"Conv1D, AUC = {:.1f}%\".format(auc(fpr_cnn,tpr_cnn)*100))\n",
    "plt.xlabel(r'True positive rate')\n",
    "plt.ylabel(r'False positive rate')\n",
    "plt.semilogy()\n",
    "plt.ylim(0.001,1)\n",
    "plt.xlim(0,1)\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(array,value):\n",
    "    idx = (np.abs(array-value)).argmin()\n",
    "    return idx, array[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation with jet mass\n",
    "This algorithm has a slight dependence on jet mass. Qualitatively, we can observe this \"mass sculpting\" by making the distributino of the jet mass with tighter and tighter requirements on the algorithm output. We can see that the original distribution of the jet mass for the background has no \"resonance\" or bump, and is basically smoothly falling. \n",
    "\n",
    "However, once a requirement is made on the algorithm output, jets with low mass or high mass are excluded at higher rates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for wp in [1.0, 0.8, 0.5, 0.1]:\n",
    "    idx, val = find_nearest(fpr_cnn, wp)\n",
    "    plt.hist(spec_array_test[:,0], bins = np.linspace(40, 200, 21), \n",
    "             weights = label_array_test[:,0]*(predict_array_cnn[:,1] > threshold_cnn[idx]),\n",
    "             alpha=0.4,density=True, label='QCD, {}% FPR cut'.format(int(wp*100)),linestyle='-')\n",
    "plt.legend()\n",
    "plt.xlabel(r'$m_{SD}$')\n",
    "plt.ylabel(r'Normalized probability')\n",
    "plt.xlim(40,200)\n",
    "\n",
    "plt.figure()\n",
    "for wp in [1.0, 0.8, 0.5, 0.1]:\n",
    "    idx, val = find_nearest(fpr_cnn, wp)\n",
    "    plt.hist(spec_array_test[:,0], bins = np.linspace(40, 200, 21), \n",
    "             weights = label_array_test[:,1]*(predict_array_cnn[:,1] > threshold_cnn[idx]),\n",
    "             alpha=0.4,density=True, label='H(bb), {}% FPR cut'.format(int(wp*100)),linestyle='-')\n",
    "plt.legend()\n",
    "plt.xlabel(r'$m_{SD}$')\n",
    "plt.ylabel(r'Normalized probability')\n",
    "plt.xlim(40,200)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist2d(spec_array_test[:,0][(label_array_test[:,0]==1) & (predict_array_cnn[:,1] > 0.1)], \n",
    "           predict_array_cnn[:,1][(label_array_test[:,0]==1) & (predict_array_cnn[:,1] > 0.1)], \n",
    "           bins=(30, 30), \n",
    "           cmap=plt.cm.jet)\n",
    "plt.colorbar()\n",
    "plt.title('QCD')\n",
    "plt.ylabel(r'Conv1D Output')\n",
    "plt.xlabel(r'$m_{SD}$')\n",
    "\n",
    "plt.figure()\n",
    "plt.hist2d(spec_array_test[:,0][(label_array_test[:,1]==1) & (predict_array_cnn[:,1] > 0.8)], \n",
    "           predict_array_cnn[:,1][(label_array_test[:,1]==1) & (predict_array_cnn[:,1] > 0.8)], \n",
    "           bins=(30, 30), \n",
    "           cmap=plt.cm.jet)\n",
    "plt.colorbar()\n",
    "plt.title('H(bb)')\n",
    "plt.ylabel(r'Conv1D Output')\n",
    "plt.xlabel(r'$m_{SD}$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependence on jet pT\n",
    "\n",
    "We can repeat the exercise this time looking at jet pT. We see that in general, higher pT jets are promoted to be more \"signal-like\" by the algorithm. This is likely due to the fact that high pT jets are over-represented in the signal sample compared to the background sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for wp in [1.0, 0.8, 0.5, 0.1]:\n",
    "    idx, val = find_nearest(fpr_cnn, wp)\n",
    "    plt.hist(spec_array_test[:,1], bins = np.linspace(300, 2000, 51), \n",
    "             weights = label_array_test[:,0]*(predict_array_cnn[:,1] > threshold_cnn[idx]),\n",
    "             alpha=0.4,density=True, label='QCD, {}% FPR cut'.format(int(wp*100)),linestyle='-')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(r'$p_{T}$')\n",
    "plt.ylabel(r'Normalized probability')\n",
    "plt.xlim(300,2000)\n",
    "\n",
    "plt.figure()\n",
    "for wp in [1.0, 0.8, 0.5, 0.1]:\n",
    "    idx, val = find_nearest(fpr_cnn, wp)\n",
    "    plt.hist(spec_array_test[:,1], bins = np.linspace(300, 2000, 51), \n",
    "             weights = label_array_test[:,1]*(predict_array_cnn[:,1] > threshold_cnn[idx]),\n",
    "             alpha=0.4,density=True, label='H(bb), {}% FPR cut'.format(int(wp*100)),linestyle='-')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(r'$p_{T}$')\n",
    "plt.ylabel(r'Normalized probability')\n",
    "plt.xlim(300,2000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
